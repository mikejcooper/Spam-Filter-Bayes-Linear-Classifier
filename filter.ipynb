{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment : Spam Filter\n",
    "## Description\n",
    "This assignment is near-final modulo some small adjustments (6 Nov '16)\n",
    "In this assignment, you will discover that in many practical machine learning problems implementing the learning algorithm is often only a small part of the overall system. Thus to get a high mark for this assignment you need to implement any of the more advanced classification techniques or clever pre-processing methods. You will find plenty of them on the internet. If you do not know where to look for them, ask Google ;-).\n",
    "\n",
    "Here your task is to build the standard (i.e. multinomial) Naive Bayes text classifier described during the lectures. You should test your program using the automatic marking software (described below), so it is critically important that it follows the specifications in detail.\n",
    "\n",
    "You will train your classifier on real-world e-mails, which you can download from here. Each training e-mail is stored in a separate file. The names of spam training e-mails start with spam, while the names of ham e-mails start with ham.\n",
    "\n",
    "## Marking criteria\n",
    "\n",
    "Part 1 (40%):\n",
    "    - Your program classifies the testing set with an accuracy significantly higher than random within 30 minutes\n",
    "    - Use very simple data preprocessing so that the emails can be read into the Naive Bayes (remove everything else other than words from emails)\n",
    "    - Write simple Naive Bayes multinomial classifier or use an implementation from a library of your choice\n",
    "    - Classify the data\n",
    "    - Report your results with a metric (e.g. accuracy) and method (e.g. cross validation) of your choice\n",
    "    - Choose a baseline and compare your classifier against it\n",
    "\n",
    "Part 2 (30%):\n",
    "    - Use some smart feature processing techniques to improve the classification results\n",
    "    - Compare the classification results with and without these techniques\n",
    "    - Analyse how the classification results depend on the parameters (if available) of chosen techniques\n",
    "    - Compare (statistically) your results against any other algorithm of your choice (use can use any library); compare and contrast results, ensure fair comparison\n",
    "\n",
    "Part 3 (30%):\n",
    "    - Calibration (15%): calibrate Naive Bayes probabilities, such that they result in low mean squared error\n",
    "    - Naive Bayes extension (15%): modify the algorithm in some interesting way (e.g. weighted Naive Bayes)\n",
    "    \n",
    "\n",
    "** Convert from .ipynb to .py : ** $ ipython nbconvert --to python filter.ipynb\n",
    "\n",
    "** BeautifulSoup: ** $ pip install beautifulsoup4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import email\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "import os, re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 1 Filtering **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'you are receiving this email because you signed up to receive one of our free reports if you would prefer not to receive messages of this type please unsubscribe by following the instructions at the bottom of this message dear investor thank you again for requesting our free special report the one stock that keeps wall street buzzing we began the motley fool in with the idea that investors like you deserved better better than wall streets alltoooften biased research better than analysts who speak in secret codes allowing them to hedge or spin any recommendation and better than what passes for full financial disclosure in big business today given a level playing field we believe that regular folks like us and you can do quite well in the stock market why put trust in conflicted information from others when you could count on your own abilities and potentially blow the pros away more than two million people visit our foolcom web site each month we spend a great deal of time at foolcom instructing people how to invest but not so much about where to invest and thats why we created the motley fool stock advisor you are cordially invited to join us as a charter subscriber to motley fool stock advisor as we focus on the great companies of the us stock market in the same honest noholdsbarred style that has made foolcom so popular with investors well bring you our best stock recommendations and other financial insights to help you achieve your financial dreams as a charter subscriber youll get the motley fool stock advisor newsletter delivered to your home each month a personal communication from us david tom gardner well tell you exactly why we believe a stock is poised for significant growth and give you all the facts to back that analysis up youll get the good the bad and the ugly on every recommendation we make so you can make each investing decision with great confidence of course theres more to our motley fool stock advisor than our monthly newsletter youll also receive our monthly betweenissue email fool flash to help you take full advantage of breaking opportunities and you can log on anytime at the motley fool stock advisor subscriberonly web site featuring current back newsletter issuesfull updates on our selected stocksq aand more best of all you can try our motley fool stock advisor entirely riskfree the motley fool community has always been based upon trust and value principles we intend to continue here so we want to give you plenty of time to decide if our motley fool stock advisor is a valuable investing tool for you you can try our motley fool stock advisor for six full months riskfree if we dont prove its worth to you it doesnt cost you a dime and you have our word on that so join us now its going to be fun its going to be exciting and its going to be an enriching experience you wont want to miss to join us riskfree as a charter subscriber to david tom gardners motley fool stock advisor simply click here now foolishly yours david tom gardner the motley fool ps click here to check out the free bonuses you get when you subscribe special reports videos online seminars all yours to enjoy and keep even if you cancel your trial subscription to our motley fool stock advisor remember you have six full months to evaluate our new service dont like what you see dont make the profits you expectthen it doesnt cost you a dime so this invitation is truly riskfree click here now how to unsubscribe we hope this email message is of value to you if however you do not wish to receive any of our future messages please unsubscribe by going to the following web address note if you unsubscribe by replying to this message please use unsubscribe or remove in the subject line thu jul'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Removes header from email if present\n",
    "\n",
    "def get_body(msg):\n",
    "    if isinstance(msg, str):\n",
    "        return msg\n",
    "    elif msg.is_multipart():\n",
    "        payloads = ''\n",
    "        for payload in msg.get_payload():\n",
    "            # if payload.is_multipart(): ...\n",
    "            payloads += ' ' + get_body(payload)\n",
    "        return payloads\n",
    "    else:\n",
    "        return msg.get_payload()\n",
    "    \n",
    "    \n",
    "# Returns text containing only words (lowercase)\n",
    "def remove_extras(text):\n",
    "    brackets = \"\\([^)]*\\)\"\n",
    "    email_address = \"([\\w.-]+)@([\\w.-]+)\"\n",
    "    web_address = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    numbers = \"\\d+\"\n",
    "    alphanumeric = \"([^\\s\\w]|_)+\"\n",
    "    whitespace = \"\\s+\"\n",
    "\n",
    "    text = re.sub(email_address, '', text)\n",
    "    text = re.sub(web_address, '', text)\n",
    "    text = re.sub(numbers, '', text)\n",
    "    text = re.sub(alphanumeric, '', text)\n",
    "    text = re.sub(whitespace, ' ', text).strip()\n",
    "    text = text.lower() # Lowercase\n",
    "    return text\n",
    "\n",
    "def simple_html_filter(html_doc):\n",
    "\n",
    "    soup = BeautifulSoup(str(html_doc), \"html.parser\")\n",
    "\n",
    "\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "        \n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def filter_stop_words(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "def filter_part1(email): \n",
    "    message = get_body(email)\n",
    "    message = simple_html_filter(message)\n",
    "    message = remove_extras(message)\n",
    "    return message\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "# Example\n",
    "\n",
    "with open('training_data/ham000.txt') as f:\n",
    "    text_test = f.read()\n",
    "\n",
    "msg = email.message_from_string(text_test)\n",
    "\n",
    "filter_part1(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part 2 filtering **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are receiving this email because you signed up to \n",
      "receive one of our free reports. If you would prefer \n",
      "not to receive messages of this type, please \n",
      "unsubscribe by following the instructions at the \n",
      "bottom of this message.\n",
      "\n",
      "\n",
      "Dear Investor,\n",
      "\n",
      "Thank you again for requesting our free special \n",
      "report, \"The One Stock that Keeps Wall Street \n",
      "BUZZING.\"\n",
      "\n",
      "We began The Motley Fool in 1993 with the idea that \n",
      "investors like you deserved better. \n",
      "\n",
      "Better than Wall Street's all-too-often biased \n",
      "research...  Better than analysts who speak in \"secret \n",
      "codes\" allowing them to hedge or spin any \n",
      "recommendation...  and better than what passes for \"full \n",
      "financial disclosure\" in big business today.\n",
      "\n",
      "Given a level playing field, we believe that regular \n",
      "folks -- like us, and you -- can do quite well in the \n",
      "stock market. Why put trust in conflicted information \n",
      "from others when you could count on your own \n",
      "abilities, and potentially blow the pros away? \n",
      "\n",
      "More than two million people visit our Fool.com web \n",
      "site each month.  We spend a great deal of time at \n",
      "Fool.com instructing people HOW to invest, but not so \n",
      "much about WHERE to invest. \n",
      "\n",
      "And that's why we created the Motley Fool Stock \n",
      "Advisor.\n",
      "\n",
      "You are cordially invited to join us as a Charter \n",
      "Subscriber to Motley Fool Stock Advisor as we focus on \n",
      "the great companies of the U.S. stock market. \n",
      "\n",
      "In the same, honest, no-holds-barred style that has \n",
      "made Fool.com so popular with investors, we'll bring \n",
      "you our best stock recommendations and other financial \n",
      "insights to help you achieve your financial dreams.\n",
      "\n",
      "As a Charter Subscriber, you'll get the Motley Fool \n",
      "Stock Advisor newsletter delivered to your home each \n",
      "month -- a personal communication from us, David & Tom \n",
      "Gardner.  We'll tell you exactly why we believe a \n",
      "stock is poised for significant growth -- and give you \n",
      "all the facts to back that analysis up.  You'll get \n",
      "the good, the bad and the ugly on every recommendation \n",
      "we make -- so you can make each investing decision \n",
      "with great confidence.\n",
      "\n",
      "Of course, there's more to our Motley Fool Stock \n",
      "Advisor than our monthly newsletter.\n",
      "\n",
      "You'll also receive our monthly, between-issue, e-mail \n",
      "Fool Flash to help you take full advantage of breaking \n",
      "opportunities.  And you can log on anytime at the \n",
      "Motley Fool Stock Advisor subscriber-only Web site, \n",
      "featuring current & back newsletter issues...full \n",
      "updates on our selected stocks...Q & A...and more.\n",
      "\n",
      "Best of all, you can try our Motley Fool Stock Advisor \n",
      "entirely RISK-FREE.\n",
      "\n",
      "The Motley Fool community has always been based upon \n",
      "trust and value -- principles we intend to continue \n",
      "here. So we want to give you plenty of time to decide \n",
      "if our Motley Fool Stock Advisor is a valuable \n",
      "investing tool for you.\n",
      "\n",
      "You can try our Motley Fool Stock Advisor for six full \n",
      "months 100% RISK-FREE. If we don't prove its worth to \n",
      "you, it doesn't cost you a dime.\n",
      "\n",
      "And you have our word on that.\n",
      "\n",
      "So join us now. It's going to be fun. It's going to be \n",
      "exciting. And it's going to be an enriching experience \n",
      "you won't want to miss.\n",
      "\n",
      "To join us, RISK-FREE, as a Charter Subscriber to \n",
      "David & Tom Gardner's Motley Fool Stock Advisor, \n",
      "simply click here now: \n",
      "http://www.ppi-orders.com/index.htm?promo_code=1UE728\n",
      "\n",
      "Foolishly yours,\n",
      "\n",
      "David & Tom Gardner\n",
      "The Motley Fool\n",
      "\n",
      "P.S. Click here \n",
      "http://www.ppi-orders.com/index.htm?promo_code=1UE728\n",
      "to check out the FREE bonuses you get when you \n",
      "subscribe.  Special Reports; videos; online seminars -\n",
      "- all yours to enjoy and keep, even if you cancel your \n",
      "trial subscription to our Motley Fool Stock Advisor.\n",
      "\n",
      "Remember, you have six full months to evaluate our new \n",
      "service.  Don't like what you see, don't make the \n",
      "profits you expect...then it doesn't cost you a dime.  \n",
      "So this invitation is truly RISK-FREE.  Click here now \n",
      "http://www.ppi-orders.com/index.htm?promo_code=1UE728\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "HOW TO UNSUBSCRIBE\n",
      "\n",
      "We hope this email message is of value to you. If \n",
      "however, you do not wish to receive any of our future \n",
      "messages, please unsubscribe by going to the following \n",
      "web address:\n",
      "\n",
      "http://www.investorplace.com/newunsubscribe.php?q=66527235-1 \n",
      "\n",
      "**Note if you unsubscribe by replying to this message \n",
      "please use Unsubscribe or Remove in the subject line. \n",
      "\n",
      "\n",
      "Thu Jul 25 15:39:46 2002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "email_address = \"([\\w.-]+)@([\\w.-]+)\"\n",
    "web_address = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "numbers = \"\\d+\"\n",
    "alphanumeric = r'[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]'\n",
    "whitespace = \"\\s+\"\n",
    "exclam = r'[!]'\n",
    "\n",
    "# Returns text containing only words (lowercase)\n",
    "def remove_extras2(text):\n",
    "    text = re.sub(email_address, ' ' + str(hash('email')) + ' ', text)\n",
    "    text = re.sub(web_address, ' ' + str(hash('web')) + ' ', text)\n",
    "    text = re.sub(numbers, ' ' + str(hash('number')) + ' ', text)\n",
    "    text = re.sub(alphanumeric, ' ' + str(hash('alphanumeric')) + ' ', text)\n",
    "    text = re.sub(whitespace, ' ', text).strip()\n",
    "    text = text.lower() # Lowercase\n",
    "    return text\n",
    "\n",
    "# Threshold between 0 and 1\n",
    "def regex_ratio(string, regex):\n",
    "    num_of_chars = len(string)\n",
    "    num_of_chars_regex = len(re.findall(regex, string.encode('utf-8')))\n",
    "    ratio = num_of_chars_regex / num_of_chars\n",
    "    return ratio\n",
    "\n",
    "def get_subject(msg):\n",
    "    if isinstance(msg['subject'], str):\n",
    "        return msg['subject']\n",
    "    \n",
    "\n",
    "# Example \n",
    "with open('training_data/ham000.txt') as f:\n",
    "    text_test = f.read()\n",
    "\n",
    "# FEATURES\n",
    "msg = email.message_from_string(text_test)\n",
    "\n",
    "regex_ratio(get_body(msg), alphanumeric)\n",
    "regex_ratio(get_body(msg), numbers)\n",
    "regex_ratio(get_body(msg), web_address)\n",
    "regex_ratio(get_body(msg), email_address)\n",
    "regex_ratio(get_body(msg), whitespace)\n",
    "regex_ratio(get_body(msg), whitespace)\n",
    "regex_ratio(get_body(msg), exclam)\n",
    "subject = get_subject(msg)\n",
    "\n",
    "# print get_body(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from geoip import geolite2\n",
    "\n",
    "def do_stemming(text):\n",
    "    stemmed = \"\"\n",
    "    for word in text.split():\n",
    "#         stemmed += PorterStemmer().stem(word) + \" \"\n",
    "        stemmed += LancasterStemmer().stem(word) + \" \"\n",
    "    return stemmed\n",
    "    \n",
    "def get_source_country(email):\n",
    "    ips = re.findall( r'[0-9]+(?:\\.[0-9]+){3}', str(email) )\n",
    "\n",
    "    if len(ips) > 0:\n",
    "        match = geolite2.lookup(ips[0])\n",
    "        if match:\n",
    "            return str(match.country)\n",
    "    return ''\n",
    "\n",
    "def filter_part2(email):\n",
    "    message = get_body(email)\n",
    "    message = simple_html_filter(message)\n",
    "    message = remove_extras2(message)\n",
    "    message = filter_stop_words(message)\n",
    "    message = do_stemming(message)\n",
    "    return message\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "# Example\n",
    "\n",
    "with open('training_data/spam036.txt') as f:\n",
    "    text_test = f.read()\n",
    "\n",
    "msg = email.message_from_string(text_test)\n",
    "\n",
    "\n",
    "# print(filter_part2(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in all documents and convert to a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "training_data = []\n",
    "training_labels = []\n",
    "training_filenames = []\n",
    "training_label_names = ['ham', 'spam']\n",
    "        \n",
    "# ham\n",
    "for filename in glob.glob('training_data/ham*.txt'):\n",
    "    f = open(filename, 'r')\n",
    "    training_data.append(f.read())\n",
    "    training_labels.append(0)\n",
    "    training_filenames.append(filename)\n",
    "\n",
    "# spam    \n",
    "for filename in glob.glob('training_data/spam*.txt'):\n",
    "    f = open(filename, 'r')\n",
    "    training_data.append(f.read())\n",
    "    training_labels.append(1)\n",
    "    training_filenames.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class PreProcessor(BaseEstimator):\n",
    "    def __init__(self, part_name):\n",
    "        self.part_name = part_name\n",
    "        \n",
    "    def transform(self, X):\n",
    "        out = []\n",
    "        for item in X:\n",
    "            if self.part_name == 'part1':\n",
    "                data = filter_part1(email.message_from_string(item))\n",
    "            elif self.part_name == 'part2':\n",
    "                data = filter_part2(email.message_from_string(item))\n",
    "            else:\n",
    "                raise Exception('unknwon part name for PreProcessor') \n",
    "            out.append(data)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "class ads(BaseEstimator):\n",
    "        \n",
    "#     def __init__(self, key):\n",
    "#         self.key = key\n",
    "        \n",
    "    def transform(self, X):\n",
    "        out = []\n",
    "        for i,vector in enumerate(X):\n",
    "            print i\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_pipe_p1 = Pipeline([('preprocess', PreProcessor('part1')),\n",
    "                    ('vect', CountVectorizer(decode_error='ignore')),\n",
    "                    ('feat', ads()),\n",
    "                    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's test using Stratified Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printIncorrectClassifications(ground_truths, predictions, names):\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if (ground_truths[i] != prediction):\n",
    "            print names[i] + ' classified as ' + training_label_names[prediction] + ' when it was ' + training_label_names[ground_truths[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "k = 10\n",
    "skf = StratifiedKFold(training_labels, n_folds=k)\n",
    "acc_per_fold = []\n",
    "\n",
    "for train_indices, test_indices in skf:\n",
    "    clf = clf_pipe_p1.fit([training_data[i] for i in train_indices], [training_labels[i] for i in train_indices])\n",
    "    predictions['stratified-nb-p1'] = clf.predict([training_data[i] for i in test_indices])\n",
    "    predictions['stratified-nb-p1-prob'] = clf.predict_proba([training_data[i] for i in test_indices])\n",
    "    acc_per_fold.append( accuracy_score([training_labels[i] for i in test_indices], predictions['stratified-nb-p1']) )\n",
    "    printIncorrectClassifications([training_labels[i] for i in test_indices], predictions['stratified-nb-p1'], [training_filenames[i] for i in test_indices])\n",
    "\n",
    "print \"Accuracy per fold:\", acc_per_fold\n",
    "print \"Average accuracy:\", np.mean(acc_per_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also test using a separate (large) batch of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def pickTestData(count=1):\n",
    "    labelsFile = open('testdata.label', 'r')\n",
    "    labeling = labelsFile.readlines()\n",
    "    random.shuffle(labeling)\n",
    "    sample = labeling[0:count]\n",
    "    return [open('test_data/'+row.split()[1], 'r').read() for row in sample], [1-int(row.split()[0]) for row in sample], ['test_data/'+row.split()[1] for row in sample]\n",
    "\n",
    "test_data, test_labels, test_filenames = pickTestData(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# we're using multinomial because it's most relevant for word counts\n",
    "\n",
    "predictions['test-nb-p1'] = clf_pipe_p1.fit(training_data, training_labels).predict(test_data)\n",
    "predictions['test-nb-p1-prob'] = clf_pipe_p1.fit(training_data, training_labels).predict_proba(test_data)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions['test-nb-p1'])\n",
    "printIncorrectClassifications(test_labels, predictions['test-nb-p1'], test_filenames)\n",
    "print \"Accuracy against test data:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plotConfusionMatrices(ground_truths, predicted):\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(ground_truths, predicted)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=training_label_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=training_label_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plotRocSimple(ground_truths, predictions):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(2):\n",
    "        fpr[i], tpr[i], _ = roc_curve([item for item in ground_truths], [item[i] for item in predictions])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ground_truths, [item[i] for item in predictions])\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(ground_truths, predictions)\n",
    "\n",
    "    df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "    auc_str = metrics.auc(fpr,tpr)\n",
    "\n",
    "    return ggplot(df, aes(x='fpr', y='tpr')) +\\\n",
    "        geom_area(alpha=0.2) +\\\n",
    "        geom_line(aes(y='tpr')) +\\\n",
    "        ggtitle(\"ROC Curve w/ AUC=%s\" % str(auc_str)) +\\\n",
    "        geom_abline(linetype='dashed')\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def plotProbaROC(ground_truths, predicted, predicted_proba):\n",
    "    y_test_predictions = predicted\n",
    "    y_test = np.array(ground_truths)\n",
    "    \n",
    "    n_classes = 2\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test, [row[i] for row in predicted_proba])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), predicted_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(training_label_names[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotConfusionMatrices(test_labels, predictions['test-nb-p1'])\n",
    "plotProbaROC(test_labels, predictions['test-nb-p1'], predictions['test-nb-p1-prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use a DummyClassifier as our baseline, in stratified cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf_pipe_dummy = Pipeline([('preprocess', PreProcessor('part1')),\n",
    "                     ('vect', CountVectorizer(decode_error='ignore')),\n",
    "#                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', DummyClassifier('uniform')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(training_labels, n_folds=k)\n",
    "acc_per_fold = []\n",
    "\n",
    "for train_indices, test_indices in skf:\n",
    "    dclf = clf_pipe_dummy.fit([training_data[i] for i in train_indices], [training_labels[i] for i in train_indices])\n",
    "    predictions['stratified-dummy-p1'] = dclf.predict([training_data[i] for i in test_indices])\n",
    "    acc_per_fold.append( accuracy_score([training_labels[i] for i in test_indices], predictions['stratified-dummy-p1']) )\n",
    "\n",
    "print \"Baseline accuracy per fold:\", acc_per_fold\n",
    "print \"Baseline average accuracy:\", np.mean(acc_per_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and against our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions['test-dummy-p1'] = clf_pipe_dummy.fit(training_data, training_labels).predict(test_data)\n",
    "predictions['test-dummy-p1-prob'] = clf_pipe_dummy.fit(training_data, training_labels).predict_proba(test_data)\n",
    "\n",
    "dummy_accuracy = accuracy_score(test_labels, predictions['test-dummy-p1'])\n",
    "\n",
    "print \"Baseline accuracy against test data:\", dummy_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotConfusionMatrices(test_labels, predictions['test-dummy-p1'])\n",
    "plotProbaROC(test_labels, predictions['test-dummy-p1'], predictions['test-dummy-p1-prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can do some more interesting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_pipe_p2 = Pipeline([('preprocess', PreProcessor('part2')),\n",
    "                     ('vect', CountVectorizer(decode_error='ignore', binary=True, max_features=10000)),\n",
    "                     ('clf', MultinomialNB(alpha=1, fit_prior=False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "skf = StratifiedKFold(training_labels, n_folds=k)\n",
    "acc_per_fold = []\n",
    "\n",
    "for train_indices, test_indices in skf:\n",
    "    clf = clf_pipe_p2.fit([training_data[i] for i in train_indices], [training_labels[i] for i in train_indices])\n",
    "    predictions['stratified-nb-p2'] = clf.predict([training_data[i] for i in test_indices])\n",
    "    acc_per_fold.append( accuracy_score([training_labels[i] for i in test_indices], predictions['stratified-nb-p2']) )\n",
    "\n",
    "print \"Accuracy per fold:\", acc_per_fold\n",
    "print \"Average accuracy:\", np.mean(acc_per_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions['test-nb-p2'] = clf_pipe_p2.fit(training_data, training_labels).predict(test_data)\n",
    "predictions['test-nb-p2-prob'] = clf_pipe_p2.fit(training_data, training_labels).predict_proba(test_data)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions['test-nb-p2'])\n",
    "\n",
    "print \"Accuracy against test data:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotConfusionMatrices(test_labels, predictions['test-nb-p2'])\n",
    "plotProbaROC(test_labels, predictions['test-nb-p2'], predictions['test-nb-p2-prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "we need to compare our part 2 algorithm to another algorithm of our choice. We'll use an SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_pipe_svm = Pipeline([('preprocess', PreProcessor('part2')),\n",
    "                    ('vect', CountVectorizer(decode_error='ignore')),\n",
    "                    ('clf', SGDClassifier(loss='log', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "\n",
    "\n",
    "predictions['test-svm-p2'] = clf_pipe_svm.fit(training_data, training_labels).predict(test_data)\n",
    "predictions['test-svm-p2-prob'] = clf_pipe_svm.fit(training_data, training_labels).predict_proba(test_data)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions['test-svm-p2'])\n",
    "\n",
    "print \"SVM accuracy against test data:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotConfusionMatrices(test_labels, predictions['test-svm-p2'])\n",
    "plotProbaROC(test_labels, predictions['test-svm-p2'], predictions['test-svm-p2-prob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best SVM tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(clf_pipe_svm, parameters, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(training_data, training_labels)\n",
    "\n",
    "print \"Best score:\", gs_clf.best_score_\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to tune the part2 parameters to see what the best value can be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'vect__binary': (True, False),\n",
    "              'vect__max_features': [None, 10000],\n",
    "              'vect__stop_words': ('english', None),\n",
    "              'clf__alpha': (0, 1),\n",
    "              'clf__fit_prior': (False, True)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(clf_pipe_p2, parameters, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(training_data, training_labels)\n",
    "\n",
    "print \"Best score:\", gs_clf.best_score_\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
